# LLM é›†æˆ (LLM Integration)

> æœ¬æ–‡æ¡£ä»‹ç» Maicraft-Next çš„ LLM ç®¡ç†å’Œè°ƒç”¨ç³»ç»Ÿ

---

## ğŸ¯ è®¾è®¡ç†å¿µ

### ç»Ÿä¸€çš„ LLM ç®¡ç†

Maicraft-Next æä¾›äº† `LLMManager` æ¥ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ LLM æä¾›å•†çš„è°ƒç”¨ï¼Œæ”¯æŒï¼š

- âœ… å¤šæä¾›å•†æ”¯æŒï¼ˆOpenAIã€Azureã€Claude ç­‰ï¼‰
- âœ… è‡ªåŠ¨æ•…éšœè½¬ç§»
- âœ… ç”¨é‡ç»Ÿè®¡å’Œè´¹ç”¨è¿½è¸ª
- âœ… é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
- âœ… æµå¼å“åº”æ”¯æŒ

---

## ğŸ“¦ æ ¸å¿ƒç»„ä»¶

### LLMManager

ç»Ÿä¸€çš„ LLM ç®¡ç†å™¨ï¼Œè´Ÿè´£ï¼š

- ç®¡ç†å¤šä¸ª LLM æä¾›å•†
- è·¯ç”±è¯·æ±‚åˆ°åˆé€‚çš„æä¾›å•†
- ç»Ÿè®¡ç”¨é‡å’Œè´¹ç”¨
- å¤„ç†é”™è¯¯å’Œé‡è¯•

### æ”¯æŒçš„æä¾›å•†

| æä¾›å•†               | çŠ¶æ€      | æ”¯æŒæ¨¡å‹                          |
| -------------------- | --------- | --------------------------------- |
| **OpenAI**           | âœ… å·²å®ç° | gpt-4, gpt-3.5-turbo, gpt-4-turbo |
| **Azure OpenAI**     | ğŸš§ å¼€å‘ä¸­ | gpt-4, gpt-35-turbo               |
| **Anthropic Claude** | ğŸš§ å¼€å‘ä¸­ | claude-3-opus, claude-3-sonnet    |

---

## ğŸ’» åŸºæœ¬ä½¿ç”¨

### åˆå§‹åŒ–

```typescript
import { LLMManager } from '@/llm/LLMManager';
import { getLogger } from '@/utils/Logger';

// ä»é…ç½®åˆ›å»º
const llmConfig = {
  default_provider: 'openai',
  openai: {
    enabled: true,
    api_key: 'sk-...',
    model: 'gpt-4',
    temperature: 0.7,
    max_tokens: 2000,
  },
};

const llmManager = new LLMManager(llmConfig, getLogger('LLM'));
```

### å‘é€èŠå¤©è¯·æ±‚

```typescript
// ç®€å•èŠå¤©
const response = await llmManager.chat([
  { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ª Minecraft AI ä»£ç†' },
  { role: 'user', content: 'æˆ‘åº”è¯¥åšä»€ä¹ˆï¼Ÿ' },
]);

console.log(response.content);
console.log(`ç”¨é‡: ${response.usage.total_tokens} tokens`);
console.log(`è´¹ç”¨: $${response.cost}`);
```

### ä½¿ç”¨å·¥å…·è°ƒç”¨

```typescript
// å¸¦å·¥å…·çš„èŠå¤©
const response = await llmManager.chat(messages, {
  tools: [
    {
      name: 'move',
      description: 'ç§»åŠ¨åˆ°æŒ‡å®šåæ ‡',
      parameters: {
        type: 'object',
        properties: {
          x: { type: 'number' },
          y: { type: 'number' },
          z: { type: 'number' },
        },
        required: ['x', 'y', 'z'],
      },
    },
  ],
  tool_choice: 'auto',
});

if (response.tool_calls) {
  for (const toolCall of response.tool_calls) {
    console.log(`è°ƒç”¨å·¥å…·: ${toolCall.name}`);
    console.log(`å‚æ•°:`, toolCall.arguments);
  }
}
```

### æµå¼å“åº”

```typescript
const stream = await llmManager.chatStream(messages);

for await (const chunk of stream) {
  process.stdout.write(chunk.content);
}
```

---

## ğŸ“Š ç”¨é‡ç»Ÿè®¡

### è·å–ç»Ÿè®¡ä¿¡æ¯

```typescript
// è·å–æ€»ç”¨é‡
const stats = llmManager.getUsageStats();
console.log(`æ€»è¯·æ±‚: ${stats.totalRequests}`);
console.log(`æ€» tokens: ${stats.totalTokens}`);
console.log(`æ€»è´¹ç”¨: $${stats.totalCost}`);

// æŒ‰æä¾›å•†ç»Ÿè®¡
console.log('OpenAI:', stats.providers.openai);

// æŒ‰æ¨¡å‹ç»Ÿè®¡
console.log('GPT-4:', stats.models['gpt-4']);
```

### æŒä¹…åŒ–ç»Ÿè®¡

```typescript
// è‡ªåŠ¨ä¿å­˜åˆ° data/usage_stats.json
await llmManager.saveUsageStats();

// åŠ è½½å†å²ç»Ÿè®¡
await llmManager.loadUsageStats();
```

---

## ğŸ”„ ä¸ Maicraft Python çš„å¯¹æ¯”

| æ–¹é¢           | Maicraft Python    | Maicraft-Next        |
| -------------- | ------------------ | -------------------- |
| **LLM è°ƒç”¨**   | openai_client æ¨¡å— | LLMManager ç»Ÿä¸€ç®¡ç†  |
| **æä¾›å•†ç®¡ç†** | å•ä¸€æä¾›å•†         | å¤šæä¾›å•†æ”¯æŒ         |
| **ç”¨é‡ç»Ÿè®¡**   | æ—                  | å®Œæ•´çš„ç»Ÿè®¡ç³»ç»Ÿ       |
| **é”™è¯¯å¤„ç†**   | åŸºç¡€é‡è¯•           | å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯• |
| **æµå¼å“åº”**   | æ”¯æŒ               | æ”¯æŒ                 |
| **å·¥å…·è°ƒç”¨**   | æ”¯æŒ               | æ”¯æŒ                 |

---

## ğŸš€ é«˜çº§ç‰¹æ€§

### 1. è‡ªåŠ¨æ•…éšœè½¬ç§»

```typescript
const llmConfig = {
  default_provider: 'openai',
  fallback_providers: ['azure', 'anthropic'],
  // ...
};

// å¦‚æœ OpenAI å¤±è´¥ï¼Œè‡ªåŠ¨å°è¯• Azure
const response = await llmManager.chat(messages);
```

### 2. è¯·æ±‚é‡è¯•

```typescript
const llmConfig = {
  openai: {
    // ...
    retry_attempts: 3,
    retry_delay: 1000,
    retry_exponential: true,
  },
};
```

### 3. é€Ÿç‡é™åˆ¶

```typescript
const llmConfig = {
  openai: {
    // ...
    rate_limit: {
      requests_per_minute: 60,
      tokens_per_minute: 90000,
    },
  },
};
```

---

## ğŸ“š åœ¨ Agent ä¸­ä½¿ç”¨

### åœ¨å†³ç­–å¾ªç¯ä¸­

```typescript
// MainDecisionLoop.ts
export class MainDecisionLoop {
  constructor(
    private state: AgentState,
    private llmManager: LLMManager,
  ) {}

  async think(): Promise<void> {
    // 1. ç”Ÿæˆ prompt
    const messages = [
      { role: 'system', content: this.getSystemPrompt() },
      { role: 'user', content: this.getUserPrompt() },
    ];

    // 2. è°ƒç”¨ LLM
    const response = await this.llmManager.chat(messages, {
      tools: this.getAvailableTools(),
      temperature: 0.7,
      max_tokens: 2000,
    });

    // 3. å¤„ç†å“åº”
    if (response.tool_calls) {
      await this.executeTools(response.tool_calls);
    }
  }
}
```

---

## ğŸš€ æœ€ä½³å®è·µ

### 1. åˆç†è®¾ç½®æ¸©åº¦

```typescript
// åˆ›é€ æ€§ä»»åŠ¡ï¼šé«˜æ¸©åº¦
const creativeResponse = await llmManager.chat(messages, {
  temperature: 0.9,
});

// ç²¾ç¡®ä»»åŠ¡ï¼šä½æ¸©åº¦
const preciseResponse = await llmManager.chat(messages, {
  temperature: 0.3,
});
```

### 2. æ§åˆ¶ token ç”¨é‡

```typescript
// é™åˆ¶å“åº”é•¿åº¦
const response = await llmManager.chat(messages, {
  max_tokens: 500, // æ§åˆ¶è´¹ç”¨
});

// æˆªæ–­å†å²æ¶ˆæ¯
const recentMessages = messages.slice(-10); // åªä¿ç•™æœ€è¿‘ 10 æ¡
```

### 3. ä½¿ç”¨æµå¼å“åº”æå‡ä½“éªŒ

```typescript
// å¯¹äºé•¿å“åº”ï¼Œä½¿ç”¨æµå¼
const stream = await llmManager.chatStream(messages);

for await (const chunk of stream) {
  // å®æ—¶æ˜¾ç¤º LLM è¾“å‡º
  console.log(chunk.content);
}
```

### 4. ç›‘æ§ç”¨é‡å’Œè´¹ç”¨

```typescript
// å®šæœŸæ£€æŸ¥è´¹ç”¨
const stats = llmManager.getUsageStats();
if (stats.totalCost > 10) {
  console.warn('è´¹ç”¨å·²è¶…è¿‡ $10');
}

// ä¿å­˜ç»Ÿè®¡
await llmManager.saveUsageStats();
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [ä»£ç†ç³»ç»Ÿ](agent-system.md) - äº†è§£ LLM å¦‚ä½•åœ¨ Agent ä¸­ä½¿ç”¨
- [æç¤ºè¯ç³»ç»Ÿ](prompt-system.md) - äº†è§£å¦‚ä½•ç”Ÿæˆé«˜è´¨é‡çš„ prompt

---

_æœ€åæ›´æ–°: 2025-11-01_
